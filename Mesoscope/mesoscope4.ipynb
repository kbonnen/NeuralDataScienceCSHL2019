{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?  =  variable name, possibly with indexing/slicing  \n",
    "??? = function / class name  \n",
    "?????? = complex expression containing variables, functions etc  \n",
    "???B??? = complex expression that must contain \"B\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using behavior to predict neural activity\n",
    "\n",
    "These groups of neurons that are not stimulus-selective, what are they doing? Maybe they are related to the behavior of the mouse?\n",
    "\n",
    "If that's the case we need some low-dimensional representation of the behavior - we can't predict using 1 million pixels of video data - that's an underconstrained problem.\n",
    "\n",
    "# 1. Load in data + behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports all the functions and data we need\n",
    "import os # os stands for \"operating system\" and includes read/write routines etc. \n",
    "import numpy as np # by far the most used library for everyday computation\n",
    "from scipy import io # this is for importing Matlab data files\n",
    "from scipy import stats # here we import a whole sub-library of stats functions\n",
    "from scipy.ndimage import gaussian_filter # here we import a single function\n",
    "from sklearn.decomposition import PCA # check out all the other dimensionality reduction methods in \"decomposition\"\n",
    "from matplotlib import pyplot as plt # all of our plotting is done with plt\n",
    "from scipy.stats import zscore\n",
    "%matplotlib inline \n",
    "\n",
    "# folder where data is \n",
    "root = '/home/neuraldata/data/meso/'\n",
    "mouse_name = 'TX39'\n",
    "\n",
    "spks = np.load(os.path.join(root, mouse_name, 'suite2p/combined/spks.npy'))\n",
    "stat = np.load(os.path.join(root, mouse_name, 'suite2p/combined/stat.npy'), allow_pickle=True)\n",
    "print('total neurons %d'%len(stat))\n",
    "\n",
    "S = zscore(spks,axis=1)\n",
    "del spks # delete spks to clear memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in behavioral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEHAVIORAL DATA\n",
    "proc   = np.load(os.path.join(root, mouse_name, 'cam1_TX39_2019_05_31_1_proc_resampled.npy'), allow_pickle=True)\n",
    "proc   = proc.item() # non-array variables need to be accessed this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the keys in proc? \n",
    "proc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the first spatial motion SVD (and second, etc)\n",
    "plt.imshow(proc['motMask'][?], cmap='bwr', vmin=-.02, vmax=.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motSVD = proc['motSVD']\n",
    "parea  = proc['pupil']['area'][0].copy()\n",
    "\n",
    "# plot the first 1000 samples of the motion SVD\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(???)\n",
    "\n",
    "# same for pupil area\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(???)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin neural and behavioral data + compute neural PCA\n",
    "\n",
    "We will bin the neural activity to do the PCA and the behavior (1 second bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin data\n",
    "tbin = int(3)\n",
    "NT = motSVD.shape[1]\n",
    "nt = int(np.floor(NT/tbin))\n",
    "motSVD -= motSVD.mean(axis=1)[:,np.newaxis]\n",
    "parea  -= np.nanmean(parea)\n",
    "parea[np.isnan(parea)] = 0\n",
    "\n",
    "beh = np.reshape(motSVD[:,:nt*tbin], (motSVD.shape[0], nt, tbin)).mean(axis=-1)\n",
    "pup = np.reshape(parea[:nt*tbin], (nt, tbin)).mean(axis=-1)\n",
    "\n",
    "# using this format bin S\n",
    "# Sbin = ?\n",
    "Sbin = np.reshape(S[:,:nt*tbin], (S.shape[0], nt, tbin)).mean(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take principal components of neural data (as previously)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# use PCA to get the top 256 neural components. The components must be spatial (by neurons)\n",
    "model_pca = ???256??? # \n",
    "U = model_pca.components_ # time components\n",
    "\n",
    "# check the shape\n",
    "print(U.shape)\n",
    "\n",
    "# check that the Us are normalized to unit norm\n",
    "plt.plot(np.sum(U**2, axis=?))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = U???Sbin # now project the binned data onto the spatial components. Figure out which transpose works! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of neural PCs\n",
    "\n",
    "Now let's predict these neural PC's using our behavioral measures. But FIRST we need to split the data into train and test segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train-test\n",
    "# * use interleaved segments *\n",
    "nlen   = 500 \n",
    "itrain = np.remainder(np.arange(nt), nlen) < ? # make 70% of the trials be training trials\n",
    "itest  = ~itrain # what is not train is test\n",
    "\n",
    "plt.plot(itrain)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you determine how I did the split above? Why might I have split into segments rather than randomly interleaving time-points?\n",
    "\n",
    "# Prediction with behavioral variables\n",
    "\n",
    "Use linear regression to perform the prediction, predict $Y$ using $X$:\n",
    "\n",
    "$$ A = (X_\\text{train}X_\\text{train}^\\top)^{-1} (X_\\text{train} Y_\\text{train}^\\top)$$\n",
    "\n",
    "$X$ is behavioral components by time, $Y$ is neural components by time. If you want to L2 regularize the linear regression:\n",
    "\n",
    "$$ A = (X_\\text{train}X_\\text{train}^\\top + \\lambda I)^{-1} (X_\\text{train} Y_\\text{train}^\\top)$$\n",
    "\n",
    "Then the prediction on time points is:\n",
    "\n",
    "$$ \\hat Y_\\text{test} = A^\\top X_\\text{test} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREDICT USING PUPIL WITH LINEAR REGRESSION\n",
    "\n",
    "X = pup[itrain]\n",
    "COV = ?????? # determine the covariance of the one-dimensional input X\n",
    "print(COV)\n",
    "\n",
    "A = 1/COV * (pup[?] @ V[:,?]) # multiply training timepoints of pupil and neural PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the explained variance on test timepoints?\n",
    "Xtest  = pup[itest]\n",
    "Vpredp = A[:,np.newaxis] * Xtest[np.newaxis,:] # predict on test points\n",
    "\n",
    "resid  = ? - ? # what are the unexplained residuals of the prediction? \n",
    "\n",
    "varexp_pupil = 1 - np.sum(resid**2, axis=1) / np.sum(V[:,itest]**2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PREDICT USING BEHAVIOR PCs \n",
    "## regularized linear regression from behavior to neural PC's\n",
    "\n",
    "# covariance of inputs\n",
    "covM  = beh[:,itrain] @ beh[:,itrain].T\n",
    "lam   = 1e6 # regularizer\n",
    "covM  = covM + lam*np.eye(beh.shape[0]) # add the regularizer\n",
    "\n",
    "# covariance between behavior and neural data\n",
    "covMV = beh[:,itrain] @ V[:,itrain].T \n",
    "\n",
    "# we need to solve for x: covM * x = covMV\n",
    "A     = np.linalg.solve(covM, covMV)\n",
    "\n",
    "Vpred = A.T @ beh[:,itest]\n",
    "resid = ? - ? # what are the unexplained residuals of the prediction? \n",
    "\n",
    "varexp = 1 - np.sum(resid**2, axis=1) / np.sum(V[:,itest]**2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of principal component activity and variance explained\n",
    "\n",
    "fig=plt.figure(figsize=(12,3))\n",
    "\n",
    "ipc = 0 ### which PC to plot\n",
    "\n",
    "ax = fig.add_axes([0.05,.05,.75,.95])\n",
    "ax.plot(V[ipc,itest])\n",
    "ax.plot(Vpred[ipc])\n",
    "ax.set_title('PC %d'%ipc)\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('activity')\n",
    "\n",
    "ax = fig.add_axes([0.9,.05, .2, .8])\n",
    "ax.semilogx(np.arange(1,varexp.size+1), varexp, color='k')\n",
    "ax.scatter(ipc+1, varexp[ipc],marker='x',color='r',s=200, lw=4, zorder=10)\n",
    "ax.semilogx(np.arange(1,varexp.size+1), varexp_pupil, color=[0.,.5,0])\n",
    "ax.text(1,0,'pupil',color=[0,.5,0])\n",
    "ax.text(10,0.2,'motion SVD')\n",
    "ax.set_xlabel('PC')\n",
    "ax.set_ylabel('fraction variance explained')\n",
    "ax.set_title('PC %d, varexp=%0.2f'%(ipc,varexp[ipc]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort neurons with rastermap\n",
    "\n",
    "Let's see how well the behavioral prediction \"looks\". First let's sort the neural activity by rastermap, then use the sorting to sort the prediction of the neural activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sorting of neurons with rastermap\n",
    "from rastermap import Rastermap\n",
    "\n",
    "# fit a rastermap model to S with one component and n_X=100, \n",
    "model = ??????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's sort by rastermap and smooth over neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMOOTHED NEURAL ACTIVITY\n",
    "# this function performs a running average filter over the first dimension of X\n",
    "def running_average(X, nbin = 100):\n",
    "    Y = np.cumsum(X, axis=0)\n",
    "    Y = Y[nbin:, :] - Y[:-nbin, :]\n",
    "    return Y\n",
    "\n",
    "# sort the neurons by the embedding dimension of the model\n",
    "isort = ???embedding???\n",
    "\n",
    "Sfilt = Sbin[isort,:] # resort Sbin\n",
    "Sfilt = running_average(Sfilt) # smoothing\n",
    "Sfilt = zscore(Sfilt, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the behavioral prediction by rastermap and smooth over neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go from PC prediction to single neuron prediction\n",
    "# U is the neurons by components PC matrix\n",
    "# Vpred is our prediction of the components\n",
    "\n",
    "Spred = ? @ ? # what is the prediction of the neurons? this matrix should number of neurons BY number of test points\n",
    "Sfilt_pred = running_average(Spred[isort, :]) # re-sort the prediction according to the rastermap, and smooth\n",
    "Sfilt_pred = zscore(Sfilt_pred, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a rastermap of activity with prediction\n",
    "\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "ax.imshow(Sfilt[:, itest[:500]], vmin = -1, vmax=3, aspect='auto', cmap = 'gray_r')\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('neurons')\n",
    "ax.set_title('neural activity (test set)')\n",
    "\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "ax.imshow(Sfilt_pred[:,:500], vmin = -1, vmax=3, aspect='auto', cmap = 'gray_r')\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('neurons')\n",
    "ax.set_title('behavior prediction (test set)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how good is the prediction across the rastermap?\n",
    "\n",
    "# Sfilt and Sfilt_pred contains neural activity/prediction smoothed across neurons.\n",
    "# Pick every 50th row . It will contain the average of 100 neurons with similar activity. \n",
    "Sp      = ?????? # choose only testing timepoints to match the size of Sp_pred\n",
    "Sp_pred = ?????? \n",
    "\n",
    "cc = np.zeros(nn)\n",
    "for j in range(nn):\n",
    "    C0 = np.corrcoef(Sp[j,:], Sp_pred[j,:])\n",
    "    cc[j] = C0[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction quality across the rastermap\n",
    "\n",
    "fig = plt.figure(figsize=(4,4))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(cc)\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_ylabel('correlation btw/ pred and true')\n",
    "ax.set_xlabel('depth on rastermap')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
