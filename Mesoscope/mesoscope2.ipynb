{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?  =  variable name, possibly with indexing/slicing  \n",
    "??? = function / class name  \n",
    "?????? = complex expression containing variables, functions etc  \n",
    "???B??? = complex expression that must contain \"B\"  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are most of the neurons \"talking about\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Load everything again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from ??? import ??? as ??? # import the plotting function\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun all the prep and processing from mesoscope1.ipynb\n",
    "mname, datexp, blk = 'TX39', '2019_05_31', '1' \n",
    "root      = '/home/neuraldata/data/meso'\n",
    "mov    = np.load(os.path.join(root, 'mov.npy')) \n",
    "iframe    = np.load(os.path.join(root, 'iframe.npy')) \n",
    "ops = np.load(os.path.join(root, 'suite2p', 'combined', 'ops.npy'), allow_pickle=True).item()\n",
    "spks = np.load(os.path.join(root, 'suite2p', 'combined', 'spks.npy'))\n",
    "stat = np.load(os.path.join(root, 'suite2p', 'combined', 'stat.npy'), allow_pickle=True)\n",
    "ypos = [stat[k]['med'][0] for k in range(len(stat))]\n",
    "xpos = [stat[k]['med'][1] for k in range(len(stat))]\n",
    "ypos, xpos = np.array(ypos), np.array(xpos)\n",
    "ypos, xpos = ypos/.5, xpos/.75 \n",
    "dt = 1 # time offset \n",
    "ivalid = iframe+dt<spks.shape[-1] # remove timepoints outside the valid time range\n",
    "iframe = iframe[ivalid]\n",
    "mov = mov[:, :, ivalid]\n",
    "S = spks[:, iframe+dt]\n",
    "S = stats.zscore(S, axis=1) # z-score the neural activity before doing anything\n",
    "print('total neurons %d'%len(stat))\n",
    "print('recorded from an area of %2.2f um by %2.2f um'%(np.ptp(ypos), np.ptp(xpos)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Run PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_model = PCA(??? = ?) # PCA is a class. Here we define a model with 100 components\n",
    "pca_model.???(S) # here we apply the model to the data\n",
    "# most models in sklearn work this way (linear regression, LASSO, ICA, tSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some components\n",
    "X = pca_model.??? # find the variable that holds the principal components\n",
    "\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.subplot(1,2,1)\n",
    "for j in range(5):\n",
    "    plt.plot(X[j] - j * .05)\n",
    "    \n",
    "plt.subplot(1,2,2)\n",
    "time_range = np.???(?, ?) # make an array containing the time range 2000 to 2500\n",
    "for j in range(5):\n",
    "    plt.plot(X[j, time_range] - j * .05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the autocorrelogram of these components \n",
    "from numpy.fft import fft, ifft\n",
    "\n",
    "# this function uses the Fourier trick to compute the autocorrelogram\n",
    "def autocorr(X, axis=-1):    \n",
    "    fX  = fft(X, axis=axis)\n",
    "    acc = np.real(ifft(fX * np.conj(fX), axis=axis))\n",
    "    return acc\n",
    "\n",
    "X   = pca_model.components_\n",
    "acc = ???(?) # apply the autocorr function to the principal components\n",
    "\n",
    "# sample some colors from the magma colormap\n",
    "num_PC = 20 # plot this many PCs\n",
    "colormap = plt.cm.magma\n",
    "colors   = colormap(np.linspace(0, 1,num_PC))\n",
    "\n",
    "# bright colors are later components\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "for j in range(num_PC):\n",
    "    plt.plot(?, c = colors[j]) # plot the first 50 timepoints of the autocorrelogram for component j\n",
    "plt.ylabel('correlation')\n",
    "plt.xlabel('time lag')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "for j in range(num_PC):\n",
    "    plt.???(np.arange(1,10000), acc[j, 1:10000], c = colors[j]) # make a plot on a logarithmic X scale\n",
    "plt.xlabel('time lag (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# color neurons according to their PC coefficients\n",
    "plt.figure(figsize=(16,9))\n",
    "for j in range(6):\n",
    "    pc0 = ???S???pca_model??? # multiply each neuron with PC #j to get its coefficient\n",
    "    pc0 /= np.max(np.abs(pc0)) # normalize components to max(abs) = 1\n",
    "    lam = np.abs(pc0) # make the size of each dot proportional to abs(coeff)\n",
    "    plt.subplot(2,3,1+j)\n",
    "    plt.scatter(xpos, -ypos, s= 10 * lam, c = pc0, cmap='bwr', ??? = ?, vmin=-1, vmax=1) # set the transparency to 0.5\n",
    "    if j==0:\n",
    "        plt.colorbar()\n",
    "    plt.title('PC %d'%(1+j))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute receptive fields for the PCs. \n",
    "Sp = model.components_\n",
    "Sp = stats.zscore(Sp, axis=1)\n",
    "\n",
    "# below is the same code as in mesoscope1.ipynb. Only one variable needs to change in one place\n",
    "ly, lx, NT = mov.shape\n",
    "X  = np.reshape(mov - 0.5, [-1, NT]) # reshape to Npixels by Ntimepoints\n",
    "X  = stats.zscore(np.abs(X), axis=1)/NT**.5  # z-score each pixel separately\n",
    "B0 = X @ S.T # get the receptive fields for each neuron\n",
    "B0 = np.reshape(B0, [mov.shape[0], mov.shape[1], -1])\n",
    "B0 = gaussian_filter(B0, [.5, .5, 0]) # smooth each receptive field a little"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "rfmax = np.max(np.abs(B0))\n",
    "for j in range(40):\n",
    "    plt.subplot(5,8,j+1)\n",
    "    rf = ? # the receptive field of component j\n",
    "    plt.imshow(rf, aspect='auto', cmap = 'bwr', vmin = -rfmax, vmax = rfmax) # plot the receptive field for each neuron\n",
    "    plt.title('PC %d'%(1+j))\n",
    "    plt.axis('off')\n",
    "    \n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two new mysteries: \n",
    "1) why do the biggest components have no receptive fields  \n",
    "2) why are the receptive fields so distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is just a demonstration, no exercise\n",
    "\n",
    "# solve the second mystery: PCA of translation-invariant data always looks like that\n",
    "# make some random noise and smooth it\n",
    "nPC = 40\n",
    "\n",
    "Xn = np.random.randn(10000, 32,32)\n",
    "Xn = gaussian_filter(Xn, [0, 2, 2])\n",
    "Xn = np.reshape(Xn, (10000, -1))\n",
    "model = PCA(n_components= nPC).fit(Xn)\n",
    "\n",
    "bPC = np.reshape(model.components_.T, (32,32, nPC))\n",
    "plt.figure(figsize=(14, 8))\n",
    "rfmax = np.max(bPC)\n",
    "for j in range(nPC):\n",
    "    plt.subplot(5,8,j+1)\n",
    "    rf = bPC[:,:,j]\n",
    "    plt.imshow(rf, aspect='auto', cmap = 'bwr', vmin = -rfmax, vmax = rfmax) # plot the receptive field for each neuron\n",
    "    plt.title('PC %d'%(1+j))    \n",
    "    plt.axis('off')\n",
    "plt.show()    \n",
    "\n",
    "# compare with DCT https://en.wikipedia.org/wiki/Discrete_cosine_transform\n",
    "del Xn    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. Run Rastermap "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rastermap re-arranges neurons in the raster plot based on similarity of activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rastermap import Rastermap\n",
    "# use Rastermap exactly how you used PCA above. \n",
    "# this time we want only ONE component, but we want 100 nodes (n_X=100)\n",
    "model = ???n_X=100???\n",
    "model.??????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort and smooth neurons along the one-dimensional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function performs a running average filter over the first dimension of X\n",
    "def running_average(X, nbin = 50):\n",
    "    Y = np.cumsum(X, axis=0)\n",
    "    Y = Y[nbin:, :] - Y[:-nbin, :] # trick to do efficient box filter\n",
    "    return Y\n",
    "\n",
    "isort = np.argsort(model.embedding[:,0])\n",
    "Sfilt = running_average(S[isort, :], 50)\n",
    "Sfilt = stats.zscore(Sfilt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the smoothed rastermap, show only timepoints \"time_range\" (take a slice)\n",
    "\n",
    "time_range = np.arange(2000,2500)\n",
    "#time_range = np.arange(12000,12500)\n",
    "\n",
    "plt.figure(figsize=(16,9))\n",
    "plt.imshow(?, vmin = -1, vmax=2, aspect='auto', cmap='gray_r')\n",
    "plt.xlabel('time points')\n",
    "plt.ylabel('sorted neurons')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HERE PLOT cells according to their rastermap embedding\n",
    "lam = model.lam # this is how well each cell correlates with assigned component\n",
    "lam = lam / np.max(lam) # normalize lam to max of 1\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "# fancy plot: scatter xpos and -ypos, color each dot by the model emmbedding position, using the colormap gist_ncar, \n",
    "# with a transparency value of 0.5, using a dot size proportional to lam\n",
    "??????\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the Rastermap components have receptive fields?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sfilt contains neural activity smoothed across neurons.\n",
    "# Pick every 50th row . It will contain the average of 100 neurons with similar activity. \n",
    "Sp = ??????\n",
    "\n",
    "ly, lx, nstim = mov.shape\n",
    "\n",
    "NN, NT = S.shape \n",
    "\n",
    "X = np.reshape(mov, [-1, NT]) # reshape to Npixels by Ntimepoints\n",
    "X = stats.zscore(np.abs(X-0.5), axis=1)/NT**.5  # z-score each pixel separately\n",
    "npix = X.shape[0]\n",
    "\n",
    "lam = .01\n",
    "ncomps = Sp.shape[0]\n",
    "B0 = X @ Sp.T # get the receptive fields for each neuron\n",
    "B0 = np.reshape(B0, [mov.shape[0], mov.shape[1], ncomps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot these receptive fields like we did before\n",
    "??????\n",
    "??????\n",
    "??????\n",
    "??????\n",
    "??????\n",
    "??????"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "1258px",
    "left": "1719px",
    "right": "20px",
    "top": "125px",
    "width": "470px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
